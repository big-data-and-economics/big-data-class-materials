---
title: Data Science for Economists
subtitle: Control variables
author: Kyle Coombs
date: "Bates College | [ECON/DCS 368](https://github.com/big-data-and-economics)"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    css: [default, metropolis, metropolis-fonts,css/my-css.css]
    lib_dir: libs
    # Run xaringan::summon_remark() for this
    #chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE) 
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 6,cache=TRUE)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, fixest, ggthemes, gganimate, scales, Cairo, magick, emo,knitr)
theme_metro <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16),
        axis.title.x = element_text(hjust = 1),
        axis.title.y = element_text(hjust = 1, angle = 0))
}
theme_void_metro <- function(x) {
  theme_void() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
theme_metro_regtitle <- function(x) {
  theme_classic() + 
  theme(panel.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        plot.background = element_rect(color = '#FAFAFA',fill='#FAFAFA'),
        text = element_text(size = 16))
}
```


# Table of contents

- [Prologue](#prologue)

- [Endogeneity/omitted variable bias](#endogeneity)

- [Control variables](#control)
    - [Example: coin value](#coin-values)

---
name: prologue
class: inverse, center, middle

# Prologue

---
# Prologue

- This week we are starting to think about causal inference

- Today, we're going to explore endogeneity a little bit

- We'll talk about how to solve it using *controls*

- As a warning: this approach is rarely the best approach to causal inference

- But it is a helpful starting point

---
# Attribution

These slides are adapted from [slides](https://nickch-k.github.io/EconometricsSlides/Week_04/Week_04_Slides_1_Controls.html) by Nick Huntington-Klein on control variables, omitted variable bias, and endogeneity.

---
# Questions?

- Ask questions about course content, problem sets, etc.

- I am trying to build this step into future lectures

---
name: endogeneity
class: inverse, center, middle

# Endogeneity and omitted variable bias

---

# Endogeneity vs. Exogeneity

- Last time I introduced .hi[exogeneity] as a property of a variable in a model

- I suggested a new saying: Correlation plus .hi[exogeneity] is causation.

- .hi[Endogeneity] is the opposite of .hi[exogeneity]

- We believe that our true model looks like this:

$$Y = \beta_0 + \beta_1X+\varepsilon$$

- Where $\varepsilon$ is *everything that determines $Y$ other than $X$*

- If $X$ is related to some of those things, we have .hi[endogeneity]

- Estimating the above model by OLS, it will mistake the effect of those *other* things for the effect of $X$, and our estimate of $\hat{\beta}_1$ won't represent the true $\beta_1$ no matter how many observations we have

---

# Endogeneity Recap

- For example, the model

$$IceCreamEating = \beta_0 + \beta_1ShortsWearing + \varepsilon$$

- The true $\beta_1$ is probably $0$. But since $Temperature$ is in $\varepsilon$ and $Temperature$ is related to $ShortsWearing$, OLS will mistakenly assign the effect of $Temperature$ to the effect of $ShortsWearing$, making it look like there's a positive effect when there isn't one

- If $Temperature$ hangs around $ShortsWearing$, but OLS doesn't know about it, OLS will give $ShortsWearing$ all the credit for $Temperature$'s impact on $IceCreamEating$

- Here we're mistakenly finding a positive effect when the truth is $0$, but it could be anything - negative effect when truth is $0$, positive effect when the truth is a bigger/smaller positive effect, negative effect when truth is positive, etc. etc.

---
name: control
class: inverse, center, middle

# Control variables

---

# To the Rescue

- One way we can solve this problem is through the use of *control variables* 

- What if $Temperature$ *weren't* in $\varepsilon$? Then we'd be fine! OLS would know how to separate out its effect from the $ShortsWearing$ effect. How do we take it out? Just put it in the model directly!

$$IceCreamEating = \beta_0 + \beta_1ShortsWearing + \beta_2Temperature + \varepsilon$$

- Now we have a *multivariable* regression model. Our estimate $\hat{\beta}_1$ will *not* be biased by $Temperature$ because we've controlled for it

<span style = "font-size: small">(probably more accurate to say "covariates" or "variables to adjust for" than "control variables" and "adjust for" rather than "control for" but hey what are you gonna do, "control" is standard)</span>

---

# To the Rescue

- So the task of solving our endogeneity problems in estimating $\beta_1$ using $\hat{\beta}_1$ comes down to us *finding all the elements of $\varepsilon$ that are related to $X$ and adding them to the model*

- As we add them, they leave $\varepsilon$ and hopefully we end up with a version of $\varepsilon$ that is no longer related to $X$

- If $cov(X,\varepsilon) = 0$ then we have an unbiased estimate!

- (of course, we have no way of checking if that's true - it's based on what we think the data generating process looks like)

---
# How?

- Controlling for a variable works by *removing variation in $X$ and $Y$ that is explained by the control variable*

- So our estimate of $\hat{\beta}_1$ is based on *just the variation in $X$ and $Y$ that is unrelated to the control variable*

- Any "accidentally-assigning-the-value-of-Temperature-to-ShortsWearing" can't happen because we've removed the effect of $Temperature$ on $ShortsWearing$ as well as the effect of $Temperature$ on $IceCreamEating$

- We're asking at that point, *holding $Temperature$ constant*, i.e. *comparing two different days with the same $Temperature$ *, how is $ShortsWearing$ related to $IceCreamEating$?

- We know we're comparing within the same $Temperature$ because we literally subtracted out all the $Temperature$ differences!

---
name: coin-values
# Example: coin value

- Let's say we have several piles of coins from a collector with different amounts of quarters and dimes

- The piles are labeled with amounts of money and the amounts of coins, but we don't know the value of the coins

- We could use regression to find out

- One thing we do know is that the collector always had at least as many dimes as quarters

- My friend Szymon Sacher suggested this example

---
# Example: coin value

```{r, echo = FALSE}
set.seed(12315)
```

```{r, echo = TRUE}
coins <- tibble(quarters = sample(0:10,10000,replace=TRUE), # sample generates a random amount of coins.
            pennies=sample(0:10, 10000, replace=TRUE),
            nickels=sample(0:10, 10000, replace= TRUE),
            error=rnorm(10000,0,.1)) %>%
  mutate(dimes = quarters + sample(0:10,10000,replace=TRUE)) %>%
  mutate(amount = 0.25*quarters + 0.10*dimes + 0.01*pennies + 0.05*nickels + error)

coins
```

---
# Straight-forward regression

```{r all-coins, echo=TRUE}
allcoins <- feols(amount~ quarters + dimes + nickels+pennies, 
  data = coins) 
etable(allcoins,fitstat=~n,digits=2,se.below=TRUE,depvar=FALSE) %>% kable(format="markdown")
```

---
# What if we remove quarters?

The coefficient on dimes changes a lot! Why?

```{r no-quarters, echo=TRUE}
noquarters <- feols(amount~ dimes + nickels+pennies, data = coins) 
etable(allcoins,noquarters,fitstat=~n,digits=2,se.below=TRUE,depvar=FALSE) %>% kable(format="markdown")
```

---
# Endogeneity of quarters

- The number of dimes was a function of quarters

- When we dropped quarters, we omitted a variable that was related to dimes and the amount of money

- So the coefficient on dimes was biased

---
# Residualize variation from quarters

1. Average of `amount` and `dimes` by quarters, i.e. the part *explained by `quarters`*
2. Subtract from `amount` and `dimes`, to get the residual *not explained by `quarters`*

```{r, echo = TRUE}
coins <- coins %>%
  group_by(quarters) %>% 
  mutate(amount_mean = mean(amount), dimes_mean = mean(dimes),
    amount_res=amount-amount_mean,dimes_res=dimes-dimes_mean) 
head(select(coins,quarters,matches('dimes|amount')))
```

---
# Residuals regression is unbiased

```{r residuals, echo = TRUE}
residuals <- feols(amount_res ~ dimes_res, data = coins) 
etable(allcoins, noquarters,residuals,
  dict=c('dimes_res'='dimes'),fitstat=~n,digits=2,se.below=TRUE,depvar=FALSE) %>% kable(format="markdown")
```

---

# Graphically with binary control $Z$

```{r, echo=FALSE, fig.width=5, fig.height=4.5}
df <- data.frame(Z = as.integer((1:200>100))) %>%
  mutate(X = .5+2*Z + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()

#Calculate correlations
before_cor <- paste("1. Start with raw data. Correlation between dimes and amount: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("6. Correlation between dimes and amount controlling for quarters: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')

#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),
  #Step 3: X de-meaned 
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),
  #Step 5: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))

p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)))+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)))+
  guides(color=guide_legend(title="W"))+
  scale_color_colorblind()+
  labs(title = 'The Relationship between Y and X, Controlling for  Z \n{next_state}')+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  theme_metro() +
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()

animate(p,nframes=200)
```

---

# Controlling

- We achieve all this just by adding the variable to the OLS equation!

- We can, of course, include more than one control, or controls that aren't binary

- Use OLS to predict $X$ using all the controls, then take the residual (the part not explained by the controls)

- Use OLS to predict $Y$ using all the controls, then take the residual (the part not explained by the controls)

- Now do OLS of just the $Y$ residuals on just the $X$ residuals

---

# A Continuous Control

```{r, out.width="700px"}
knitr::include_graphics("linear_control.gif")
```

---

# What do we get?

- We can remove some of the relationship between $X$ and $\varepsilon$

- Potentially all of it, making $\hat{\beta}_1$ us an *unbiased* (i.e. correct on average, but sampling variation doesn't go away!) estimate of $\beta_1$

- Maybe we can also get some estimates of $\beta_2$, $\beta_3$... but be careful, they're subject to the same identification and endogeneity problems!

- Often in econometrics we focus on getting *one* parameter, $\hat{\beta}_1$, exactly right and don't focus on parameters we haven't put much effort into identifying

---
# Summary

- We can remove endogeneity by adding omitted variables into our regression model if:
    1. We know/correctly assume what they are
    2. We can measure them

- This works by removing the part $X$ and $Y$ that is related to the omitted variable, $Z$

- This is fairly common, but often inadequate approach to causal inference

- Sometimes it is the best we can do though! 

```{r gen_pdf, include = FALSE, cache = FALSE, eval = TRUE}
infile=knitr::current_input() %>% stringr::str_replace('.Rmd', '.html')
#print(infile)
pagedown::chrome_print(input = infile, timeout = 100)
```