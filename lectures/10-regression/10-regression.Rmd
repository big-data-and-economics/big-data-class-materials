---
title: "Big Data and Economics"
subtitle: "Regression analysis in R"
author:
  name: Kyle Coombs
  affiliation: Bates College | [DCS/ECON 368](https://github.com/big-data-and-economics/big-data-class-materials)
# date: Lecture 6  #"`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    theme: journal
    highlight: haddock
    # code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: true
    keep_tex: false ## Change to true if want keep intermediate .tex file
    css: css/preamble.css ## For multi-col environments
  pdf_document:
    latex_engine: xelatex
    toc: true
    dev: cairo_pdf
    # fig_width: 7 ## Optional: Set default PDF figure width
    # fig_height: 6 ## Optional: Set default PDF figure height
    includes:
      in_header: tex/preamble.tex ## For multi-col environments
    extra_dependencies: ["float", "booktabs", "longtable"]
    pandoc_args:
        --template=tex/mytemplate.tex ## For affiliation field. See: https://bit.ly/2T191uZ
always_allow_html: true
urlcolor: blue
mainfont: cochineal
sansfont: Fira Sans
# monofont: Fira Code ## Although, see: https://tex.stackexchange.com/q/294362
## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=300)
```

This lecture covers the bread-and-butter tool of applied econometrics and data science: regression analysis. This adapted from work by [Grant McDermott](https://raw.githack.com/uo-ec607/lectures/master/08-regression/08-regression.html). Today is focused on practical skills of using regression tools with R to do some analysis. I am not working through major econometric theory of regression analysis and inference. We're instead focusing on the practical skills of producing regression tables, interpreting coefficients, standardizing variables, and understanding the different types of standard errors.

There are also many R packages I'm leaving out! There are tons of ways to do regressions in R. I'll present just a few. 

## Software requirements

### R packages 

It's important to note that "base" R already provides all of the tools we need for basic regression analysis. However, we'll be using several additional packages today, because they will make our lives easier and offer increased power for some more sophisticated analyses.

- New: **fixest**, **broom**, **modelsummary**, **vtable**
- Already used: **tidyverse**, **hrbrthemes**, **listviewer**, **tidycensus**

A convenient way to install (if necessary) and load everything is by running the below code chunk.

```{r libs_print, cache=FALSE, eval=FALSE}
## Load and install the packages that we'll be using today
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, fixest, vtable, broom, modelsummary,tidycensus)

## My preferred ggplot2 plotting theme (optional)
theme_set(theme_minimal())
```
```{r libs, cache=FALSE, message=FALSE, echo=FALSE}
## Load and install the packages that we'll be using today
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, fixest, vtable, broom, modelsummary,tidycensus,kableExtra)
## My preferred ggplot2 plotting theme (optional)
theme_set(theme_minimal())
```

While we've already loaded all of the required packages for today, I'll try to be as explicit about where a particular function is coming from, whenever I use it below. 

Something else to mention up front is that we are using the Opportunity Atlas today for our regressions. Some of these regressions will be a bit simple, but the point is to show you how to run them in R with data that are meaningful. We'll also use the `fips_codes` data from the **tidycensus** package to merge in county names and state abbreviations. 

The Opportunity Atlas file lives on GitHub, so we'll download it from there (*Note*: I am using the githack URL. You could also sync your fork, pull to your cloned repo, and navigate to `lectures/10-regression/` and the file is there too.). I've also amended the file to make it smaller so it can be easily pushed to GitHub. _Note: This is bad practice generally, do not store data files on GitHub unless you have a really good reason to do so._

```{r opp_atlas}
# Create opp atlas object by reading in a CSV off the internet. 
opp_atlas <- read_csv("https://raw.githubusercontent.com/big-data-and-economics/big-data-class-materials/main/lectures/10-regression/opp_atlas_amended.csv") 

#Quickly renaming fips data to make it easier to merge
fips <- fips_codes %>% 
  rename(state_abb=state,
    state=state_code,
    county_name=county,
    county=county_code) %>%
  mutate(across(c(state,county),as.numeric))
```

```{r look-at-data,eval=FALSE}
# Look at file to refresh your memory
View(opp_atlas)
```

```{r opp_atlas2}
opp_atlas <- opp_atlas %>%
  left_join(fips) %>% # Join together the data
  rename(kfr_p25=kfr_pooled_pooled_p25,
    kfr_white_p25=kfr_white_pooled_p25)  # Rename so you have to type less later.
```

## Regression basics

### The `lm()` function

R's base workhorse command for running regression models is the built-in `lm()` function. The "**lm**" stands for "**l**inear **m**odels" and the syntax is very intuitive. The syntax is straight-forward to run a simple regression, but it can be a bit cumbersome to run more complex models. 

```r
lm(y ~ x1 + x2 + x3 + ..., data = df)
```

Where `y` is the dependent variable, `x1`, `x2`, `x3`, etc. are the independent variables, and `df` is the data frame that contains these variables. You'll note that the `lm()` call includes a reference to the data source. We [covered this](https://raw.githack.com/uo-ec607/lectures/master/04-rlang/04-rlang.html#global_env) in our earlier lecture on R language basics and object-orientated programming, but the reason is that many objects (e.g. data frames) can exist in your R environment at the same time. So we need to be specific about where our regression variables are coming from --- even if `opp_atlas` is the only data frame in our global environment at the time. 

Here's an example using the Opportunity Atlas data. We'll run a simple bivariate regression of the 1990 poverty rate on income mobility for children in the 25th percentile. 

```{r ols_lm}
ols_lm = lm(kfr_p25 ~ poor_share1990, data = opp_atlas)
ols_lm
```

You might immediately notice that the coefficient seems wrong. A higher share that are poor in 1990 is associated with greater income mobility? We'll get there in a second. 

First, let's note some limitations of `lm()`. If you want to throw in fixed effects, non-standard errors, or any other fancy stuff, you'll need to bring in other packages. So why not just teach you the same syntax, but with a more flexible package? That sounds better. Let's do that. 

### The **fixest** package and `feols()` function

The **fixest** package ([link](https://lrberge.github.io/fixest/)) is a powerful and flexible package for running linear models in R created by Laurent Bergé. It's particularly well-suited for high-dimensional fixed effects models, but it also has a bunch of other neat features. It is **extremely** fast too because it leverages some more complicated econometric theorems to speed up computation, reduce memory usage, and parallelize the computation. (We'll do parallelization later.)

**fixest** has many different linear models built in, but `feols()` is what is used for ordinary least squares (OLS) regression. The syntax is very similar to `lm()`, but with a few extra bells and whistles. 

```{r feols, message=FALSE}
#library(fixest) # already loaded
ols_fixest = feols(kfr_p25 ~ poor_share1990, data = opp_atlas)
ols_fixest
```

The output of `ols_lm` and `ols_fixest` differs a little, but check -- are the coefficients and standard errors the same? Always check if a new tool gives the same results as the simpler tool before diving into further. 

#### What's in these objects? 

First, let's talk about the object we have. The resulting **fixest** (and **lm**) object is pretty terse, but that's only because it buries most of its valuable information --- of which there is a lot --- within an internal list structure. If you're in RStudio, you can inspect this structure by typing `View(ols_fixest)` or simply clicking on the "ols_fixest" object in your environment pane. Doing so will prompt an interactive panel to pop up for you to play around with. That approach won't work for this knitted R Markdown document, however, so I'll use the `listviewer::jsonedit()` function (used in the APIs lecture) instead.

```{r ols1_str, message=F, out.width="100%", out.height="10%"}
# View(ols_fixest) ## Run this instead if you're in a live session
listviewer::jsonedit(ols_fixest, mode="view") ## Better for R Markdown
```

As we can see, this `ols_fixest` object has a bunch of important slots... containing everything from the regression coefficients, to vectors of the residuals and fitted (i.e. predicted) values, to the rank of the design matrix, to the input data, etc. etc. To summarise the key pieces of information, we can use the --- *wait for it* --- generic `summary()` function. This will look pretty similar to the default regression output from Stata that many of you will be used to.

```{r ols1_summ}
summary(ols_fixest)
```

We can then dig down further by extracting a summary of the regression coefficients:

```{r ols1_coefs}
summary(ols_fixest)$coefficients
```

#### Multiple regression 

The `feols()` function can handle multiple regression models unlike `lm()`. It can split regressions by different dependent variables, independent variables, fixed effects, and subsets all in one. I need to briefly create an indicator for region to show you multiple subset regression.

```{r ols1_multi}
# Indicator for south created.
opp_atlas <- opp_atlas %>% 
  mutate(insouth= ifelse(state_abb %in% c("AL", "AR", "FL", "GA", "KY", "LA", "MS", "NC", "OK", "SC", "TN", "TX", "VA", "WV"), "South", "North"))

ols_multi <- feols(c(kfr_p25, kfr_white_p25) ~  
  sw(poor_share1990,ann_avg_job_growth_2004_2013),
  fsplit=~insouth, # split by region
  data = opp_atlas)
etable(ols_multi) # explained below
```

One function call produced 12 regressions and you could technically do thousands or even millions at once. This is a very powerful tool for running many regressions at once. Use it wisely.

### Get "tidy" regression coefficients with the `broom` package

While it's easy to extract regression coefficients via the `summary()` function, in practice I often use the **broom** package ([link ](https://broom.tidyverse.org/)) to do so. **broom** has a bunch of neat features to convert statistical objects like regressions into "tidy" data frames. This is especially useful because regression output is so often used as an input to something else, e.g. a plot of coefficients or marginal effects. Here, I'll use `broom::tidy(..., conf.int = TRUE)` to coerce the `ols_fixest` regression object into a tidy data frame of coefficient values and key statistics.

```{r ols1_tidy}
# library(broom) ## Already loaded
tidy(ols_fixest, conf.int = TRUE)
```

Again, I could now pipe this tidied coefficients data frame to a **ggplot2** call, using saying `geom_pointrange()` to plot the error bars. Feel free to practice doing this yourself now, but we'll get to some explicit examples further below.

**broom** has a couple of other useful functions too. For example, `broom::glance()` summarises the model "meta" data (R<sup>2</sup>, AIC, etc.) in a data frame.

```{r ols1_glance}
glance(ols_fixest)
```

By the way, if you're wondering how to export regression results to other formats (e.g. LaTeX tables), don't worry: We'll [get to that](#regression-tables) at the end of the lecture.

### Regressing on subsetted data

Our simple model isn't particularly good; the R<sup>2</sup> is only `r I(round(glance(ols_fixest)$r.squared, 3))`. Let's check for outliers by plotting the data.

```{r outlier, message=FALSE, echo=FALSE, warning=FALSE}
opp_atlas %>%
  ggplot(aes(x=poor_share1990, y=kfr_p25)) +
  geom_point(alpha=0.5) +
  geom_point(
    data = opp_atlas %>% filter(kfr_p25==max(kfr_p25, na.rm=T) | poor_share1990==max(poor_share1990, na.rm=T)), 
    col="red",size=3
    ) +
  annotate("text", x = 90, y = 99, label = "Outliers!", col = 'red') +
  annotate("text", x = 0, y = 95, label = "Outliers!", col = 'red') +
  annotate("text", x = 95, y = 0, label = "Outliers!", col = 'red') +
  labs(
    title = "Spot the outlier",
    caption = "Remember: Always plot your data..."
  )
```

It looks like NAs were replaced with 99 by someone... ahem... someone who wants to remind you to plot your data and check for quirks like this. Maybe we should exclude outliers from our regression? You can do this in two ways: 

1. Subset (`filter()`) the original data frame directly in the `feols()` call.
2. Create a new data frame and then regress

#### 1) Subset directly in the `lm()` call

Running a regression directly on a subsetted data frame is equally easy.

```{r ols2a}
ols_fixest_sub = feols(kfr_p25 ~ poor_share1990, data = opp_atlas %>% filter(kfr_p25!=99 & poor_share1990!=99))
summary(ols_fixest_sub)
```

#### 2) Create a new data frame

Recall that we can keep multiple objects in memory in R. So we can easily create a new data frame that excludes Jabba using, say, **dplyr** ([lecture](https://raw.githack.com/big-data-and-economics/big-data-class-materials/main/lectures/05-tidyverse/05-tidyverse.html)) or **data.table** ([lecture](https://raw.githack.com/big-data-and-economics/big-data-class-materials/main/lectures/05-datatable/05-datatable.html). For these lecture notes, I'll stick with **dplyr** commands. But it would take just a little elbow grease (with help from ChatGPT or CoPilot) to switch to **data.table** if you prefer.

```{r ols2}
opp_atlas =
  opp_atlas %>% 
  filter(kfr_p25!=99 & poor_share1990!=99)

ols_fixest_filter = feols(kfr_p25 ~ poor_share1990, data = opp_atlas)
summary(ols_fixest_filter)
```


The overall model fit is much improved by the exclusion of this outlier, with R<sup>2</sup> increasing to `r I(round(glance(ols_fixest_filter)$r.squared, 3))`. Still, we should be cautious about throwing out data. Another approach is to handle or account for outliers with statistical methods. Which provides a nice segue to nonstandard errors.

## Nonstandard errors

Dealing with statistical irregularities (heteroskedasticity, clustering, etc.) is a fact of life for empirical researchers. However, it says something about the economics profession that a random stranger could walk uninvited into a live seminar and ask, "How did you cluster your standard errors?", and it would likely draw approving nods from audience members. 

The good news is that there are *lots* of ways to get nonstandard errors in R. For many years, these have been based on the excellent **sandwich** package ([link](http://sandwich.r-forge.r-project.org/articles/sandwich.html)) and **estimatr** package ([link](https://declaredesign.org/r/estimatr/articles/getting-started.html)). However, I'll demonstrate using the **fixest** package, which has a built-in `vcov` argument that allows you to easily specify different types of standard errors.

### Robust standard errors

One of the primary reasons that you might want to use robust standard errors is to account for heteroskedasticity. What is heteroskedasticity, well it is when the variance of the error term is not constant across observations. This is a problem because it violates one of the key assumptions of OLS regression, namely that the error term is homoskedastic (i.e. constant variance).^[See [Causal Inference: The Mixtape](https://mixtape.scunning.com/02-probability_and_regression#robust-standard-errors) for more details.] I present an example below with fake data (cause it is easier than forcing it out of real data.)

```{r heteroskedasticity}
# Create an example of heteroskedasticity

# This creates a simple regression, but with variance that increases with x
hetero_df <- tibble(
  x = rnorm(1000),
  y = 1 + 2 * x + rnorm(1000, sd = 1 + 5 * abs(x))
)

# Plot the data
hetero_df %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(
    title = "Heteroskedasticity",
    subtitle = "The variance of the error term is not constant across observations",
    caption = "Source: Gen AI (GitHub CoPilot) helped me write this example, but I still used my brain too."
  )

```

There are many ways to deal with heteroskedasticity, but one of the most common is to use robust standard errors or "HC1" heteroskedasticity-consistent standard errors. **fixest** and `feols()` has an argument, `vcov` (for variance-covariance matrix) that you can set as "HC1" or "hetero" to generate standard errors. I'll illustrate below! 

```{r ols1_robust}

ols_robust = feols(kfr_p25 ~ poor_share1990, data = opp_atlas, vcov = "HC1")
ols_robust = feols(kfr_p25 ~ poor_share1990, data = opp_atlas, vcov = "hetero")
# tidy(ols1_robust, conf.int = TRUE) ## Could tidy too
ols_robust
```

If you use the package **estimatr**, there is a function `lm_robust()`, which defaults to using Eicker-Huber-White robust standard errors, commonly referred to as "HC2" standard errors. You can easily specify alternate methods using the `se_type = ` and `vcov` argument.^[See the [package documentation](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes) for a full list of options.] For example, you can specify Stata robust standard errors (`reg y x, robust`) if you want to replicate code or results from that language. (See [here](https://declaredesign.org/r/estimatr/articles/stata-wls-hat.html) for more details on why this isn't the default and why Stata's robust standard errors differ from those in R and Python. tl;dr: A few years ago several people realized Stata was reporting different SEs than they expected.) See Grant's notes for more advanced treatment of robustness.

### Clustered standard errors

Another way that standard errors can violate homoskedasticity is for the error to be "clustered" by groups in the data. A classic example is students in the same classroom will all have the same teacher and so their learning outcomes will be correlated. For example, all of you have the same teacher (me), so you will all learn the same coding habits. This is a problem because it violates the assumption that the error term is independent across observations. Here's an example with the Opportunity Atlas data.

```{r error_clusters}
# Let's look at just a few states, so we can easily see clusters
filter(opp_atlas, state_abb %in% c('FL','NY','TX','ME')) %>% 
  ggplot(aes(x=poor_share1990,y=kfr_p25)) + 
  geom_point(aes(col=state_abb)) + 
  geom_smooth(method='lm') +
  labs(
    col = 'State',
    title = "Error clusters",
    subtitle = "The error term is correlated within clusters",
    caption = "Source: GitHub CoPilot helped me write this example, but I still used my brain"
  )
```

Clustered standard errors is an issue that most commonly affects panel data. As such, I'm going to hold off discussing clustering until we get to the [panel data section](#high-dimensional-fes-and-multiway-clustering) below. But here's a quick example of clustering with `fixest::feols`:

```{r ols1_robust_clustered, warning = FALSE}
fixest_cluster <- feols(kfr_p25 ~ poor_share1990, data = opp_atlas, cluster = ~state)
```

### Manipulating variables

#### Standardize variables

Regression coefficients can be tricky to understand! The units may be strange or the coefficients may be hard to interpret. One way to make coefficients easier to interpret is to manipulate the variables to be more informative. For example, you might want to standardize the variables to return a correlation coefficient. 

The statistical equation for a correlation coefficient is:

$$r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\hat{\sigma}^2_x,\hat{\sigma}^2_y} $$

Where $x_i$ and $y_i$ are the individual observations, and $\bar{x}$ and $\bar{y}$ are the means of the variables. The $\hat{\sigma}^2_x$ and $\hat{\sigma}^2_y$ are the sample variances of the variables. Let's change the variables `kfr_p25` and `poor_share1990` to be standardized and then run a regression. 

```{r standardize}
opp_atlas <- opp_atlas %>% 
  mutate(
    kfr_p25_std = (kfr_p25-mean(kfr_p25))/sd(kfr_p25),
    poor_share1990_std = (poor_share1990-mean(poor_share1990))/sd(poor_share1990)
  )

```

Take a look at the variables to see if they are standardized. 

```{r reg_correlation_coefficient}
fixest_corr <- feols(kfr_p25_std ~ poor_share1990_std, data = opp_atlas)
summary(fixest_corr)
```

Is that a correlation coefficient? Maybe? That'd be neat! How can we find out? Well use the `cor()` function and see if the coefficients are the same. 

```{r correlation_coefficient}
cor(opp_atlas$kfr_p25, opp_atlas$poor_share1990)
```

They are! This is a handy trick to standardized two radically different variables to tell you about the overall correlation when you're less interested in the individual coefficients. This regression trick also returns standard errors which are really nice to have. 

#### Convert variables to logs

Another useful trick is to log variables! You can do this directly or use the `log()` function. 

```{r log_variables}
tidy(feols(log(kfr_p25) ~ log(poor_share1990), data = opp_atlas))
```

## Dummy variables and interaction terms

For the next section, we'll need to create a dummy variable. We'll create two: one for whether a county is in the South and one for having positive job growth.

```{r in_south,message=F,warning=F}
# Get the regions and do minor cleaning
opp_atlas <- mutate(opp_atlas,
  in_south = ifelse(state_abb %in% c("AL", "AR", 
    "FL", "GA", "KY", "LA", "MS", "NC", "OK",
     "SC", "TN", "TX", "VA", "WV"), "South", "North"))
```

### Dummy variables as *factors*

Dummy variables are a core component of many regression models. However, these can be a pain to create in some statistical languages, since you first have to tabulate a whole new matrix of binary variables and then append it to the original data frame. In contrast, R has a very convenient framework for creating and evaluating dummy variables in a regression: Simply specify the variable of interest as a [factor](https://r4ds.had.co.nz/factors.html).^[Factors are variables that have distinct qualitative levels, e.g. "male", "female", "non-binary", etc.]

Here's an example where we explicitly tell R that "in_south" is a factor. Since I don't plan on reusing this model, I'm just going to print the results to screen rather than saving it to my global environment.

```{r ols_dv}
summary(feols(kfr_p25 ~ poor_share1990 + as.factor(in_south), data = opp_atlas))
```

Okay, I should tell you that I'm actually making things more complicated than they need to be with the heavy-handed emphasis on factors. R is "friendly" and tries to help whenever it thinks you have misspecified a function or variable. While this is something to be [aware of](https://rawgit.com/grantmcdermott/R-intro/master/rIntro.html#r_tries_to_guess_what_you_meant), normally It Just Works^TM^. A case in point is that we don't actually *need* to specify a string (i.e. character) variable as a factor in a regression. R will automatically do this for you regardless, since it's the only sensible way to include string variables in a regression.

```{r ols_dv2}
## Use the non-factored version of "in_south" instead; R knows it must be ordered
## for it to be included as a regression variable
summary(feols(kfr_p25 ~ poor_share1990 + in_south, data = opp_atlas))
```

What happens if I use job growth quartile as a dummy variable? First, let's check the values of this value. It is uneven because we removed missing values before (a bit haphazardly, I might add.)

```{r job_growth_quartile}
table(opp_atlas$job_growth_quartile)
```

```{r ols_dv3,eval=TRUE}
summary(feols(kfr_p25 ~ poor_share1990 + job_growth_quartile, data = opp_atlas))
```

We can fix this by telling R that job_growth_quartile is a factor.

```{r ols_dv4,eval=TRUE}
summary(feols(kfr_p25 ~ poor_share1990 + as.factor(job_growth_quartile), data = opp_atlas))
```

### Interaction effects

Like dummy variables, R provides a convenient syntax for specifying interaction terms directly in the regression model without having to create them manually beforehand.^[Although there are very good reasons that you might want to modify your parent variables before doing so (e.g. centering them). As it happens, Grant McDermott [has strong feelings](https://twitter.com/grant_mcdermott/status/903691491414917122) that interaction effects are most widely misunderstood and misapplied concept in econometrics. However, that's a topic for another day.] You can use any of the following expansion operators:

- `x1:x2` "crosses" the variables (equivalent to including only the x1 × x2 interaction term)
- `x1/x2` "nests" the second variable within the first (equivalent to `x1 + x1:x2`; more on this [later](#nestedmarg))
- `x1*x2` includes all parent and interaction terms (equivalent to `x1 + x2 + x1:x2`) 

As a rule of thumb, if [not always](#nestedmarg), it is generally advisable to include all of the parent terms alongside their interactions. This makes the `*` option a good default. 

For example, we might wonder whether the relationship between a location's income mobility for children in the 25th percentile and its 1990 poverty rate differs by region. That is, we want to run a regression of the form,

$$KFR_{P25} = \beta_0 + \beta_1 D_{South} + \beta_2  + \beta_3 D_{South} \times 1990 Poverty Share$$

To implement this in R, we simply run the following,

```{r ols_ie}
ols_ie = feols(kfr_p25 ~ in_south * poor_share1990, data = opp_atlas)
summary(ols_ie)
```

## Presentation

### Tables

#### Regression tables

There are loads of [different options](https://hughjonesd.github.io/huxtable/design-principles.html) here. 

Fixest actually provides its own table function, `etable()` (for "estimation table"), which is a bit like Stata's `esttab` or `outreg` command. It's a bit more flexible than the default `summary()` function, but it only works with **fixest** list objects. 

```{r etable}
etable(ols_fixest,ols_fixest_filter,ols_robust,fixest_cluster,fixest_corr,ols_ie)
```

Hmm... how can I clean that up a bit? Well `etable()` has a number of options including `dict` where you can assign variable names. 

```{r etable clean}

dict = c("poor_share1990"="Poverty Rate (1990)",
          'poor_share1990_std'='Std. Poverty Rate (1990)',
          'kfr_p25'='Income Mobility (25th Percentile)',
          'kfr_p25_std'='Std. Income Mobility (25th Percentile)',
          'in_south'='In South')

etable(ols_fixest,ols_fixest_filter,ols_robust,fixest_cluster,fixest_corr, dict = dict)

```

There are tons of other tools to use too! 

As a note, here's what you need to do in Rmarkdown to get it to work in a PDF format. You must specify to output as a latex file, then you can report "asis" and ask `etable()` to output a tex file. (See PDF for that.)

```{r etable_pdf,results='asis'}
etable(ols_fixest,ols_fixest_filter,ols_robust,fixest_cluster,fixest_corr, dict = dict, tex=TRUE)
```

Alternatively, you can save this file to a tex file and then include it in your LaTeX document using `input()` or `include()`. 

```{r, eval=FALSE}
etable(ols_fixest,ols_fixest_filter,ols_robust,fixest_cluster,fixest_corr, dict = dict, tex=TRUE,file="regression_table.tex")
```

Other neat tricks with `etable()` is that you can specify a "note" and "label" for the table. Also, you can change standard error calculations for all the models at once. 

```{r etable_caption}
etable(ols_fixest,ols_fixest_filter,ols_robust,fixest_cluster,fixest_corr, dict = dict, notes="This is a caption", label="tab:regression_table",se='IID')
```

#### Other tools

Another great tool for creating and exporting regression tables is the **modelsummary** package ([link](https://vincentarelbundock.github.io/modelsummary)). It is extremely flexible and handles all manner of models and output formats. **modelsummary** also supports automated coefficient plots and data summary tables, which I'll get back to in a moment. The [documentation](https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html) is outstanding and you should read it, but here is a bare-boned example just to demonstrate.

```{r msum, warning=FALSE}
# library(modelsummary) ## Already loaded

## Note: msummary() is an alias for modelsummary() add variable names
msummary(list("lm"=ols_lm, "fixest"=ols_fixest,"filter"=ols_fixest_filter,"robust"=ols_robust,"cluster"=fixest_cluster),
         stars=TRUE, ## Output type
         coef_map = c("poor_share1990"="Poverty Rate (1990)",
          "(Intercept)"="Constant"),
          title="Relationship between Average Income Percentile in 2015 of Children born in 25th percentile and 1990 poverty rate") ## Rename coefficients

```

</br>
One nice thing about **modelsummary** is that it plays very well with R Markdown and will automatically coerce your tables to the format that matches your document output: HTML, LaTeX/PDF, RTF, etc. Of course, you can also [specify the output type](https://vincentarelbundock.github.io/modelsummary/#saving-and-viewing-output-formats) if you aren't using R Markdown and want to export a table for later use. Finally, you can even specify special table formats like *threepartable* for LaTeX and, provided that you have called the necessary packages in your preamble, it will render correctly (see example [here](https://github.com/grantmcdermott/lecturenotes)).

#### Summary tables

A variety of summary tables --- balance, correlation, etc. --- can be produced by the companion set of `modelsummary::datasummary*()` functions. Again, you should read the [documentation](https://vincentarelbundock.github.io/modelsummary/articles/datasummary.html) to see all of the options. But here's an example of a very simple balance table using a subset of our "humans" data frame.

```{r datsum}
datasummary_balance(~ in_south,
                    data = opp_atlas %>% 
                    dplyr::select(kfr_p25:ann_avg_job_growth_2004_2013,in_south))
```

</br>
Another package that I like a lot in this regard is **vtable** ([link](https://nickch-k.github.io/vtable)). Not only can it be used to construct descriptive labels like you'd find in Stata's "Variables" pane, but it is also very good at producing the type of "out of the box" summary tables that economists like. For example, here's the equivalent version of the above balance table.

```{r st1}
# library(vtable) ## Already loaded

## An additional argument just for formatting across different output types of
## this .Rmd document
otype = ifelse(knitr::is_latex_output(), 'return', 'kable')

## vtable::st() is an alias for sumtable()
vtable::st(opp_atlas %>% 
  dplyr::select(kfr_p25:ann_avg_job_growth_2004_2013, in_south),
   group = 'in_south', 
   out = otype)
```

</br>

Lastly, Stata users in particular might like the `qsu()` and `descr()` functions from the lightning-fast **collapse** package ([link](https://sebkrantz.github.io/collapse)).

### Figures

#### Coefficient plots

We've already worked through an example of how to extract and compare model coefficients [here](#comparing-our-model-coefficients). I use this "manual" approach to visualizing coefficient estimates all the time. However, our focus on **modelsummary** in the preceding section provides a nice segue to another one of the package's features: [`modelplot()`](https://vincentarelbundock.github.io/modelsummary/articles/modelplot.html). Consider the following, which shows both the degree to which `modelplot()` automates everything and the fact that it readily accepts regular **ggplot2** syntax.

```{r modplot, warning=FALSE}
# library(modelsummary) ## Already loaded
mods = list('No clustering' = summary(ols_fixest, se = 'standard'))

modelplot(mods) +
  ## You can further modify with normal ggplot2 commands...
  coord_flip() + 
  labs(
    title = "Relationship between Pov Rate (1990) and Income Mobility",
    subtitle = "Comparing fixed effect models"
    )
```

## Further resources

- [Ed Rubin](https://twitter.com/edrubin) has outstanding [teaching notes](http://edrub.in/teaching.html) for econometrics with R on his website. This includes both [undergrad-](https://github.com/edrubin/EC421S19) and [graduate-](https://github.com/edrubin/EC525S19)level courses. Seriously, check them out.
- Several introductory texts are freely available, including [*Introduction to Econometrics with R*](https://www.econometrics-with-r.org/) (Christoph Hanck *et al.*), [*Using R for Introductory Econometrics*](http://www.urfie.net/) (Florian Heiss), and [*Modern Dive*](https://moderndive.com/) (Chester Ismay and Albert Kim).
- [Tyler Ransom](https://twitter.com/tyleransom) has a nice [cheat sheet](https://github.com/tyleransom/EconometricsLabs/blob/master/tidyRcheatsheet.pdf) for common regression tasks and specifications.
- [Itamar Caspi](https://twitter.com/itamarcaspi) has written a neat unofficial appendix to this lecture, [*recipes for Dummies*](https://itamarcaspi.rbind.io/post/recipes-for-dummies/). The title might be a little inscrutable if you haven't heard of the `recipes` package before, but basically it handles "tidy" data preprocessing, which is an especially important topic for machine learning methods. We'll get to that later in course, but check out Itamar's post for a good introduction.
- I promised to provide some links to time series analysis. The good news is that R's support for time series is very, very good. The [Time Series Analysis](https://cran.r-project.org/web/views/TimeSeries.html) task view on CRAN offers an excellent overview of available packages and their functionality.
- Lastly, for more on visualizing regression output, I highly encourage you to look over Chapter 6 of Kieran Healy's [*Data Visualization: A Practical Guide*](https://socviz.co/modeling.html). Not only will learn how to produce beautiful and effective model visualizations, but you'll also pick up a variety of technical tips.
