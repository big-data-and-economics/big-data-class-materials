<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kyle Coombs" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data Science for Economists
]
.subtitle[
## Randomized Controlled Trials and Simulations
]
.author[
### Kyle Coombs
]

---


&lt;style type="text/css"&gt;
@media print {
  .has-continuation {
    display: block !important;
  }
}
pre {
  max-height: 350px;
  overflow-y: auto;
}

pre[class] {
  max-height: 100px;
}
&lt;/style&gt;



# Table of contents

- [Introduction to RCTs](#rcts)
- [Direct Rental Assistance Example](#rental-assistance)
- [Power Analysis](#power-analysis)
- [Simulation](#simulation)

---
name: rcts
class: inverse, center, middle

# Introduction to RCTs

---

# Introduction to RCTs

- Randomized Controlled Trials (RCTs) are a gold standard in research for establishing causal relationships.
- They allow researchers to control potential outcomes by randomly assigning subjects to treatment and control groups.
- This randomization helps ensure that any differences in outcomes can be attributed to the treatment itself.

---
name: rental-assistance
class: inverse, center, middle

# Direct Rental Assistance Example

---

# Direct Rental Assistance Example

- On problem set, several of note DRA could help people have more control over their rent and ability to move.

- That is a great idea and better yet it is empirically testable.

- Let's say we give every low-income household in the U.S. direct rental assistance.

- We can't just give it to all of them and look at what they do:
  
1. That's too expensive
2. We wouldn't see the counterfactual

- High income households are a bad counterfactual too

- Given we likely won't give it to everyone immediately (budget constraints):

- Why not randomize the stages?

---
class: white-slide



.hi-slate[54 households of varying starting rents (the color of the squares)]
&lt;img src="20-rcts_files/figure-html/house_plot1-1.png" style="display: block; margin: auto;" /&gt;

---
class: white-slide
count: false

.hi-orange[Randomly assign treatment] in period 1
&lt;img src="20-rcts_files/figure-html/house_plot2-1.png" style="display: block; margin: auto;" /&gt;

Give to the rest in period 2

---
# Balance test

- After a randomization, we can check if the treated and control groups are balanced.

- We can do this by looking at the distribution of the covariates between the two groups.

- If there is no balance, we may need to re-randomize.

- Easiest way is to do a two-sample t-test


Table: Two-sample t-test balance test between treated and control households (hundreds of $)

|variable           | estimate|   p.value|      Treated|    Untreated|
|:------------------|--------:|---------:|------------:|------------:|
|rent               |     0.30| 0.7367448| 1.131335e+01| 1.101247e+01|
|income             |  1736.65| 0.6323698| 5.173245e+04| 4.999580e+04|
|age                |    -0.01| 0.9969637| 4.464286e+01| 4.465385e+01|
|family_size        |     0.19| 0.5572725| 2.571429e+00| 2.384615e+00|
|years_in_residence |    -0.91| 0.2420371| 4.421429e+00| 5.330769e+00|
|black              |     0.10| 0.4501555| 6.785714e-01| 5.769231e-01|



---
# Easiest analysis two-sample t-test

- Two-sample t-test is the easiest analysis to compare means between two groups


``` r
ttest &lt;- t.test(rent ~ trt, data = filter(house_did,after==1))
ttest %&gt;% 
  tidy() %&gt;% 
  mutate(estimate = -round(estimate,2),
    Difference = case_when(p.value&lt;0.01 ~ paste0(estimate,"***"),
                           p.value&lt;0.05 ~ paste0(estimate,"**"),
                           p.value&lt;0.1 ~ paste0(estimate,"*"),
                           TRUE ~ as.character(estimate)),
     Treated = estimate2,
     Untreated = estimate1) %&gt;% 
  select(Difference, Untreated, Treated) %&gt;% 
  kable(caption = "Two-sample t-test of neighborhood quality between treated and control households (hundreds of $)")
```



Table: Two-sample t-test of neighborhood quality between treated and control households (hundreds of $)

|Difference | Untreated|  Treated|
|:----------|---------:|--------:|
|-4.7***    |  13.01247| 8.313347|

---
name: power-analysis
class: inverse, center, middle

# Power Analysis

---

# Power Analysis

- We have statistical significance, but how much can we trust it?

- How likely is it that this is a fluke and the true effect is actually zero? 
  - A false positive or Type I error, let's say `\(\alpha = Pr(\text{reject } H_0 | H_0 \text{ is true})\)`

- If we had failed to reject the null hypothesis of 0, how likely is it that the true effect is actually 0?
  - A false negative or Type II error, let's say `\(\beta = Pr(\text{fail to reject } H_0 | H_0 \text{ is false})\)`

- Statistical power is defined as: `\(1 - \beta\)`

- Power analysis helps understand the probability of these errors -- given some assumptions:

- We can use power analysis to determine the sample size required to detect an effect of a given size with a certain degree of confidence.

- Alternatively, we can use power analysis to determine the power of a given sample size to detect an effect of a given size.

---
# How does it work? 

- Scientists determine the type I error rate, `\(\alpha\)` -- that's 

- But type II errors depend on a ton of factors:
  - Effect size
  - Sample size
  - Variability of the outcome
  - Type I error rate

---
# Effect: Magnitude of difference

&lt;img src="20-rcts_files/figure-html/densities-1.png" style="display: block; margin: auto;" /&gt;

---
# Standard deviation: Variability

&lt;img src="20-rcts_files/figure-html/densities-sd-1.png" style="display: block; margin: auto;" /&gt;

---
# Sample Size:

&lt;img src="20-rcts_files/figure-html/sample_size-check-1.png" style="display: block; margin: auto;" /&gt;

---
# Type I error rate: `\(\alpha=0.05\)` vs. `\(\alpha=0.001\)`

&lt;img src="20-rcts_files/figure-html/type_i_error-1.png" style="display: block; margin: auto;" /&gt;

---
# How do we calculate power?

The power calculations vary by test statistic, which can make them pretty tedious!

T-test:

$$
1-\beta \approx 1 - \Phi \left(1.64 - \frac{\text{Effect}}{\text{SD}/\sqrt{n}}\right)
$$

where `\(\Phi\)` is the cumulative distribution function of the standard normal distribution.

You can invert that to solve for the sample size for a given power:

$$
n \approx \left(\frac{\text{Effect}}{\text{SD}}\right)^2 \left(1.64 + \Phi^{-1}(1-\beta)\right)^2
$$

---
# Using tools like `pwr`

Thankfully, you don't have to do this by hand! `pwr` is a package that can help you calculate power for a variety of tests (but not all of them).


``` r
pwr::pwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, 
  type = "two.sample", alternative = "two.sided")
```

```
## 
##      Two-sample t test power calculation 
## 
##               n = 63.76561
##               d = 0.5
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
## 
## NOTE: n is number in *each* group
```

- A software called G*Power can do a lot more, but it is limited by what is pre-programmed

- But what happens when it gets more complex?

---
# Power Analysis with complexity

- So before we did a two-way t-test and got a difference of

- But that may be insufficient if the households were not perfectly balanced too start.

- Plus, maybe household rents are correlated over time (need clustering)

- Then we'd do a diff-in-diff analysis


``` r
feols(rent ~ after*trt | hh + t, data = house_did,cluster=~hh) %&gt;% 
  etable() %&gt;%
  kable(caption = "Diff-in-diff analysis of neighborhood\nquality between treated and control households (hundreds of $)")
```



Table: Diff-in-diff analysis of neighborhood
quality between treated and control households (hundreds of $)

|                |.                 |
|:---------------|:-----------------|
|Dependent Var.: |rent              |
|                |                  |
|after x trt     |-5.0*** (1.1e-15) |
|Fixed-Effects:  |----------------- |
|hh              |Yes               |
|t               |Yes               |
|_______________ |_________________ |
|S.E.: Clustered |by: hh            |
|Observations    |108               |

---
# Power Analysis

- Uh oh, how do you do power analysis for diff-in-diff with clustering?

- The calculation is going to more complex

- What if we just simulated a bunch of fake datasets and saw how often we commit a type II error?

- We could set the standard deviation, effect size, sample size, and significance level (`\(\alpha\)`)

- We could then simulate a bunch of fake datasets and see how often we reject the null hypothesis

- This is a simulation!

---
name: simulation-ai
class: inverse, center, middle

# Simulation

---
# Simulation Basics in R

- Simulations are a powerful tool for modeling complex systems and testing hypotheses.

- In R, you can generate random data using functions like `rnorm()`, `runif()`, and `rbinom()`.

- Simulations often involve running multiple iterations to assess variability and robustness.

- Results can be analyzed using summary statistics, visualizations, and statistical tests.

- I've literally been using them all semester

---
# Simple Simulation Example

Here I make a dataset of 1000 observations using `tibble()` and define various parameters of the model. Let me grab the p-value from the model.


``` r
# Load libraries
set.seed(123)
n &lt;- 54
effect_size &lt;- -.5
untreated_before_mean &lt;- 10
treated_before_mean &lt;- 11
untreated_after_mean &lt;- 12
treated_after_mean &lt;- treated_before_mean+effect_size
sigma &lt;- 5
alpha &lt;- 0.05

# Make a tibble (or data.frame) to contain our data
tib &lt;- tibble(
  # Create 54 units observed over 2 periods
  unit = rep(1:n, each = 2),
  time = rep(1:2, n),
  # Treatment indicator for 27 units
  treatment = rep(c(0,1), each = n/2)[unit],
  # Post period indicator
  post = time == 2
) %&gt;%
  # Create the outcome with treatment effect
  mutate(
    # Defining untreated and treated units from the averages
    # With standard errors that vary by unit
    Y = case_when(treatment==1 &amp; post==1 ~ treated_after_mean,
               treatment==1 &amp; post==0 ~ treated_before_mean,
               treatment==0 &amp; post==1 ~ untreated_after_mean,
               treatment==0 &amp; post==0 ~ untreated_before_mean)+ 
               rnorm(n*2, mean=0, sd=sigma)
  )

# Fit the model
model &lt;- feols(Y ~ treatment*post, data = tib, cluster = ~unit)

# Get the p-value
p_value &lt;- tidy(model) %&gt;% filter(term=="treatment:postTRUE") %&gt;% pull(p.value)

# Print the p-value
sig &lt;- ifelse(p_value &lt; alpha, "Significant", "Not Significant")

# Print the results
cat("P-value:", p_value, "\n")
```

```
## P-value: 0.2243965
```

``` r
cat("Significance:", sig, "\n")
```

```
## Significance: Not Significant
```

---
# Simulate it 2000 times!

R has a bunch of great ways to iterate, there's `for` loops, `lapply` and `purrr::map`. I'll cover them more latter, for now let's use a `for` loop to simulate it 2000 times!&lt;sup&gt;1&lt;/sup&gt;


``` r
# initialize a vector to store the p-values
coef_results &lt;- c()
sig_results &lt;- c()

for (i in 1:20) {
  tib &lt;- tibble(
    unit = rep(1:n, each = 2),
    time = rep(1:2, n),
    treatment = rep(c(0,1), each = n/2)[unit],
    post = time == 2
  ) %&gt;%
    mutate(
      Y = case_when(treatment==1 &amp; post==1 ~ treated_after_mean,
                treatment==1 &amp; post==0 ~ treated_before_mean,
                treatment==0 &amp; post==1 ~ untreated_after_mean,
                treatment==0 &amp; post==0 ~ untreated_before_mean)+ 
                rnorm(n*2, mean=0, sd=sigma)
    )
  
  model &lt;- feols(Y ~ treatment*post, data = tib, cluster = 'unit')
  
  coef_results[i] &lt;- tidy(model) %&gt;% filter(term=="treatment:postTRUE") %&gt;% pull(estimate)
  sig_results[i] &lt;- tidy(model) %&gt;% filter(term=="treatment:postTRUE") %&gt;% pull(p.value) &lt;= alpha
}
```

The mean of the statistically significant results is 0.2, meaning we have `mean(sig_results, na.rm=FALSE)*100`% statistical power.

.footnote[&lt;sup&gt;1&lt;/sup&gt; `for` loops are the worst way to do things in `R`, but this syntax is the most intuitive.]

---
# Check the distribution of coefficients

&lt;img src="20-rcts_files/figure-html/sim_2000_plot-1.png" style="display: block; margin: auto;" /&gt;

---
# What are are stat. significant?

&lt;img src="20-rcts_files/figure-html/sim_2000_sig_plot-1.png" style="display: block; margin: auto;" /&gt;

---
# Then fiddle with the parameters

- How does power change with sample size?

- How does power change with the effect size?

- How does power change with the significance level?

- How does power change with the standard deviation of the outcome?

- How does power change with the standard deviation of the predictor?

- What if we added square terms? Or made it an RDD design or a diff-in-diff design?

- How does power change with the number of covariates?

- How does power change with the number of clusters?

---
# Note: Always set your seed!

- The seed is the starting point for the random number generator

- If you don't set it, you'll get a different answer every time you run the code

- This is why we set the seed to 123


``` r
set.seed(123)
print(rnorm(10))
```

```
##  [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499
##  [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197
```

``` r
set.seed(123)
print(rnorm(10))
```

```
##  [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499
##  [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197
```

- Got the same thing!

---

# AI in Simulation

- AI can enhance simulation by automating the writing and execution of simulation code.
- With carefully worded instructions, AI can generate complex simulations efficiently.
- This approach can improve the robustness and reliability of research findings by exploring a wider range of scenarios.
- I might ask it:

&gt; I want to simulate a diff-in-diff design with a treatment effect of -0.5, a sample size of 54, and a significance level of 0.05. How many times will I reject the null hypothesis if I run this simulation 2000 times?

- It could return code or a simulation function

---
# Now go try this yourself!

- Go out and simulate the power of your favorite design!

- This is a great way to understand the power of your design!

- It's also a great way to understand the behavior of your estimator

- And it's a great way to understand the behavior of your data

- Or really to understand any tricky bit of econometrics or data analysis


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
